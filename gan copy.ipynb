{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e806/anaconda3/envs/naturalspeech/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "類別名稱: ['Samoyed', 'Siberian_husky', 'Maltese_dog', 'malamute', 'beagle', 'Eskimo_dog']\n",
      "訓練資料集大小: 1185\n",
      "訓練資料批次大小: torch.Size([64, 3, 333, 500])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import utils\n",
    "import matplotlib.pyplot as plt\n",
    "# 指定資料集的路徑\n",
    "dataset_path = './Images'\n",
    "\n",
    "# 指定要選擇的資料夾名稱\n",
    "selected_breeds = ['Samoyed', 'Siberian_husky', 'Maltese_dog', 'malamute', 'beagle','Eskimo_dog']\n",
    "\n",
    "# 定義資料轉換\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((333, 500)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 自定義資料集類別\n",
    "class SelectedBreedDataset(ImageFolder):\n",
    "    def __init__(self, root, transform=None, selected_breeds=None):\n",
    "        super(SelectedBreedDataset, self).__init__(root, transform)\n",
    "        if selected_breeds:\n",
    "            self.samples = [\n",
    "                (path, class_idx) for path, class_idx in self.samples\n",
    "                if self.classes[class_idx] in selected_breeds\n",
    "            ]\n",
    "            self.targets = [class_idx for _, class_idx in self.samples]\n",
    "            self.classes = selected_breeds\n",
    "\n",
    "# 載入訓練資料集\n",
    "train_dataset = SelectedBreedDataset(root=dataset_path, transform=transform, selected_breeds=selected_breeds)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# 獲取類別名稱\n",
    "class_names = train_dataset.classes\n",
    "print(\"類別名稱:\", class_names)\n",
    "\n",
    "# 檢查資料集的大小\n",
    "print(\"訓練資料集大小:\", len(train_dataset))\n",
    "\n",
    "# 檢查資料載入器的批次大小\n",
    "for images, labels in train_loader:\n",
    "    print(\"訓練資料批次大小:\", images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器網路\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, num_classes=10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 512 * 4 * 4),\n",
    "            #nn.BatchNorm1d(512 * 4 * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Unflatten(1, (512, 4, 4)),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        print(\"Input shape before batch normalization:\", x.shape)\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10,device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.label_emb = nn.Embedding(num_classes, 512).to(self.device)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, stride=2, padding=1).to(self.device)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2, padding=1).to(self.device)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2, padding=1).to(self.device)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2, padding=1).to(self.device)\n",
    "        self.conv5 = nn.Conv2d(256, 512, 4, stride=2, padding=1).to(self.device)\n",
    "        self.model = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self.conv2,\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self.conv3,\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self.conv4,\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self.conv5,\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.fc = nn.Linear(512, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, img, labels):\n",
    "        x = self.model(img)\n",
    "        c = self.label_emb(labels)\n",
    "        \n",
    "        # 動態調整 self.out 層的輸入大小\n",
    "        flattened_size = x.shape[1]\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(flattened_size + 512, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            self.fc,\n",
    "            self.sigmoid\n",
    "        ).to(self.device)\n",
    "        \n",
    "        x = torch.cat([x, c], 1)\n",
    "        validity = self.out(x)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/200 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 真實圖像\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m real_validity \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m d_real_loss \u001b[38;5;241m=\u001b[39m criterion(real_validity, torch\u001b[38;5;241m.\u001b[39mones_like(real_validity))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 生成圖像\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/naturalspeech/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, img, labels)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     36\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(flattened_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m),\n\u001b[1;32m     37\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.2\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid\n\u001b[1;32m     40\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, c], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m validity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validity\n",
      "File \u001b[0;32m~/anaconda3/envs/naturalspeech/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/naturalspeech/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/naturalspeech/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/naturalspeech/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "# 訓練開始前建立目錄\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "# 設定超參數\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "num_epochs = 200\n",
    "batch_size = images.size()[0]\n",
    "\n",
    "# 初始化生成器和判別器\n",
    "generator = Generator(latent_dim,len(class_names))\n",
    "discriminator = Discriminator(len(class_names))\n",
    "\n",
    "# 設定設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# 設定優化器\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# 定義損失函數\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 生成固定的雜訊和標籤,用於生成圖像\n",
    "fixed_noise = torch.randn(100, latent_dim, device=device)\n",
    "fixed_labels = torch.randint(0, len(class_names), (100,), device=device)\n",
    "\n",
    "# 訓練迴圈\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    for i, (imgs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)):\n",
    "        print(\"Batch size:\", imgs.shape[0])\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 訓練判別器\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # 真實圖像\n",
    "        real_validity = discriminator(real_imgs, labels)\n",
    "        d_real_loss = criterion(real_validity, torch.ones_like(real_validity))\n",
    "        \n",
    "        # 生成圖像\n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        gen_labels = torch.randint(0, len(class_names), (batch_size,), device=device)\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        fake_validity = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = criterion(fake_validity, torch.zeros_like(fake_validity))\n",
    "        \n",
    "        # 反向傳播與優化\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # 訓練生成器\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # 生成圖像\n",
    "        fake_validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = criterion(fake_validity, torch.ones_like(fake_validity))\n",
    "        \n",
    "        # 反向傳播與優化\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # 打印訓練狀態\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[Epoch {epoch+1}/{num_epochs}] [Batch {i+1}/{len(train_loader)}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "    \n",
    "    # 在每個 epoch 結束時生成並儲存圖像\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(fixed_noise, fixed_labels).detach().cpu()\n",
    "        img_grid = torchvision.utils.make_grid(gen_imgs, nrow=10, normalize=True)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(img_grid.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Epoch {epoch+1}\")\n",
    "        plt.savefig(f\"generated_images/epoch_{epoch+1}.png\", bbox_inches='tight')\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturalspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
